{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'data = loader.load()\\nindex_creator = VectorstoreIndexCreator()\\ndocsearch = index_creator.from_loaders([loader])\\n'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import kor\n",
    "import spacy\n",
    "import pandas as pd\n",
    "import en_core_sci_md\n",
    "from langchain.llms import OpenAI\n",
    "import numpy as np\n",
    "import os\n",
    "import tiktoken\n",
    "from langchain.indexes import VectorstoreIndexCreator\n",
    "from langchain.chains import RetrievalQA\n",
    "from kor import create_extraction_chain\n",
    "# from langchain.document_loaders.csv_loader import CSVLoader\n",
    "\n",
    "from negspacy.negation import Negex\n",
    "from negspacy.termsets import termset\n",
    "\n",
    "#Define the OpenAI API key\n",
    "aiKey = \"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\"\n",
    "os.environ[\"OPENAI_API_KEY\"] = aiKey\n",
    "\n",
    "\n",
    "#Use langchain openai model\n",
    "llm = OpenAI(openai_api_key=aiKey, model=\"text-davinci-003\", temperature=0.1) # type: ignore\n",
    "nlp = spacy.load(\"en_core_sci_md\" )\n",
    "ts = termset(\"en_clinical\")\n",
    "\n",
    "nlp.add_pipe(\"entity_linker\")\n",
    "nlp.add_pipe(\n",
    "    \"negex\",\n",
    "    config={\n",
    "        \"neg_termset\":ts.get_patterns()\n",
    "    }\n",
    ")\n",
    "\n",
    "csvPath = r\"C:\\Users\\jreno\\Documents\\Projects\\Mycoach Health\\NOTEEVENTS\\NOTEEVENTS.csv\"\n",
    "# loader = CSVLoader(csvPath)\n",
    "\n",
    "'''data = loader.load()\n",
    "index_creator = VectorstoreIndexCreator()\n",
    "docsearch = index_creator.from_loaders([loader])\n",
    "'''\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pydantic\n",
    "from pydantic import Field, BaseModel\n",
    "from typing import Optional\n",
    "from kor import Object, Text, Number\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.chains import create_extraction_chain_pydantic\n",
    "from langchain.prompts import ChatPromptTemplate\n",
    "\n",
    "#Format below given clinicalAbbr as key value pairs between abbr and description\n",
    "abbr = { \"CSF\": \"cerebrospinal fluid\", \n",
    "                \"CSU\": \"catheter stream urine sample\",\n",
    "                \"CT scan\": \"computerised tomography scan\",\n",
    "                \"CVP\": \"central venous pressure\", \n",
    "                \"CXR\": \"chest X-ray\",\n",
    "                \"DNACPR\": \"do not attempt cardiopulmonary resuscitation\",\n",
    "                \"DNAR\": \"do not attempt resuscitation\",\n",
    "                \"DNR\": \"do not resuscitate\", \"Dr\": \"doctor\",\n",
    "                \"DVT\": \"deep vein thrombosis\",\n",
    "                \"Dx\": \"diagnosis\",\n",
    "                \"ECG\": \"electrocardiogram\", \n",
    "                \"ED\": \"emergency department\"\n",
    "            }\n",
    "\n",
    "class patient(pydantic.BaseModel):\n",
    "    patientName: str = Field(\n",
    "        description=\"The name of the patient, First Name and Last Name concatenated\",\n",
    "        examples = \"[**First Name (NamePattern)**]\"\n",
    "    )\n",
    "    diagnosisData: str = Field(\n",
    "        description=\"The summary of the diagnosis of the patient or the final diagnosis of the patient\",\n",
    "    )\n",
    "    time : Optional[str] = Field(\n",
    "        description=\"The time of the diagnosis of the patient\",\n",
    "    )\n",
    "    clinicalAbbr : Optional[str] = Field(\n",
    "        description=\"The clinical abbreviations present in the diagnosis of the patient\",\n",
    "        #Write an example for the same including a clinical abbreviation for the model to detect\n",
    "        examples = abbr\n",
    "    )\n",
    "    medsData : Optional[str] = Field(\n",
    "        description=\"The medications of the patient prescribed or administered to the patient, might have abbreviations\",\n",
    "        examples = [\"Paracetamol 75mcg p.o.\" , \"Aspirin 81mg p.i. q.d.\"]\n",
    "    )\n",
    "    dischargeData : Optional[str] = Field(\n",
    "        description=\"The summary of the discharge of the patient, including the medical status of the patient\",\n",
    "        example= \"The patient was able to oxygenate on room air at 93% at the time of discharge.\"\n",
    "    )\n",
    "    medicalScans : Optional[list] = Field(\n",
    "        description=\"The summary of the medical scans of the patient, including but not limited to CT Scans, MRIs and X-Ray Scans\",\n",
    "        example = [\"\"]\n",
    "    )\n",
    "    additionalInfo : Optional[str] = Field(\n",
    "        description=\"Any additional information about the patient, including but not limited to the patient's medical history, allergies, etc.\",\n",
    "        example = \"The patient has had a history of asthma.\"\n",
    "    )\n",
    "    \n",
    "\n",
    "#Printing the patient's name and diagnosis\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Define the prompting template for the extraction and summarisation of patient data\\\n",
    "from langchain.prompts import ChatPromptTemplate, Prompt, PromptTemplate, MessagesPlaceholder\n",
    "from langchain.tools import PubmedQueryRun\n",
    "\n",
    "\n",
    "#Import the libraries whcih ahvent been imported but are being invoked\n",
    "from langchain.tools.base import ToolException\n",
    "\n",
    "from langchain.agents import AgentType, initialize_agent, AgentExecutor\n",
    "from langchain.memory import ConversationBufferMemory, ChatMessageHistory\n",
    "from langchain.embeddings.openai import OpenAIEmbeddings\n",
    "\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.tools import Tool\n",
    "\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from typing import Dict, Tuple\n",
    "\n",
    "from langchain.chains import RetrievalQA\n",
    "import openai\n",
    "\n",
    "modelName = \"text-embedding-ada-002\"\n",
    "embed = OpenAIEmbeddings(\n",
    "            client=openai,\n",
    "            openai_api_key=aiKey\n",
    "        )\n",
    "\n",
    "class LLMConfig():\n",
    "    model = \"text-davinci-003\"\n",
    "    llm = llm\n",
    "\n",
    "def setup_memory() -> Tuple[Dict, ConversationBufferMemory]:\n",
    "    \"\"\"\n",
    "    Sets up memory for the open ai functions agent.\n",
    "    :return a tuple with the agent keyword pairs and the conversation memory.\n",
    "    \"\"\"\n",
    "    agent_kwargs = {\n",
    "        \"extra_prompt_messages\": [MessagesPlaceholder(variable_name=\"memory\")],\n",
    "    }\n",
    "    memory = ConversationBufferMemory(memory_key=\"memory\", return_messages=True)\n",
    "\n",
    "    return agent_kwargs, memory\n",
    "\n",
    "def setupAgent() -> AgentExecutor :\n",
    "    pubmed = medTool\n",
    "    cfg = LLMConfig()\n",
    "    tools = [\n",
    "        Tool.from_function(\n",
    "            func=pubmed.run, \n",
    "            name='PubMed',\n",
    "            description='Useful tool for querying medical publications'\n",
    "        )\n",
    "    ]\n",
    "    agent_kwargs, memory = setup_memory()\n",
    "    \n",
    "    return initialize_agent(\n",
    "        tools, \n",
    "        llm, \n",
    "        agent=AgentType.OPENAI_FUNCTIONS, \n",
    "        verbose=False, \n",
    "        agent_kwargs=agent_kwargs,\n",
    "        memory=memory\n",
    "    )\n",
    "\n",
    "#Tokenizer len function\n",
    "'''def tiktoken_len(text):\n",
    "    tokens = tokenizer.encode(\n",
    "        text,\n",
    "        disallowed_special=()\n",
    "    )\n",
    "    return len(tokens)'''\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#Define the prompt template for the extraction and summarisation of patient data\n",
    "prompt_template = '''\n",
    "                    Imagine you are a medical professional with years of experience in various diseases, their clinical terminologies, how they worsen or improve over time and an expert in pharmaceuticals and medication.\n",
    "                    You have access to tools such as PubMedQuery which will help you connect to Pubmed and retrieve relevant clinical references according to the patient diagnosis. Also, you have access to the context and the structured data of the patient's clinical\n",
    "                    records. You have been asked to summarise the patient's condition, diagnosis, medication, relevant procedures and try to augment the data at hand to get useful insights and a better understanding.\n",
    "                    Both the data variables are JSON objects which are fed to you (might be embedded or not), make sure you interpret them correctly.\n",
    "                    \n",
    "                    Context: This is the given context for the aprticular patient data being taught to you. {contextData}\n",
    "                    Patient Data: This is the given patient data for you to handle and infer from based on the context, step-by-step semantic analysis. {patientData}\n",
    "                    \n",
    "                    Break down the information you have step-by-step and summarise it.\n",
    "                '''\n",
    "   \n",
    "medTool = PubmedQueryRun()\n",
    "prompt = PromptTemplate(template=prompt_template, input_variables=[\"contextData\", \"patientData\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValidationError",
     "evalue": "1 validation error for StuffDocumentsChain\n__root__\n  document_variable_name text was not found in llm_chain input_variables: ['contextData', 'patientData'] (type=value_error)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValidationError\u001b[0m                           Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[10], line 17\u001b[0m\n\u001b[0;32m     15\u001b[0m chain \u001b[39m=\u001b[39m create_extraction_chain_pydantic(patient, llm)\n\u001b[0;32m     16\u001b[0m embedder \u001b[39m=\u001b[39m SpacyEmbeddings(nlp\u001b[39m=\u001b[39men_core_sci_md\u001b[39m.\u001b[39mload())\n\u001b[1;32m---> 17\u001b[0m summaryChain \u001b[39m=\u001b[39m load_summarize_chain(llm, chain_type\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39mmap_reduce\u001b[39;49m\u001b[39m\"\u001b[39;49m, return_intermediate_results\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m, map_prompt\u001b[39m=\u001b[39;49mprompt, combine_prompt\u001b[39m=\u001b[39;49mprompt)\n\u001b[0;32m     20\u001b[0m count, lim \u001b[39m=\u001b[39m \u001b[39m0\u001b[39m, \u001b[39m10\u001b[39m\n\u001b[0;32m     22\u001b[0m \u001b[39mfor\u001b[39;00m item \u001b[39min\u001b[39;00m df:\n\u001b[0;32m     23\u001b[0m     \u001b[39m#Pass the items through nlp pipeline \u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\jreno\\Documents\\Projects\\Mycoach Health\\venv\\Lib\\site-packages\\langchain\\chains\\summarize\\__init__.py:146\u001b[0m, in \u001b[0;36mload_summarize_chain\u001b[1;34m(llm, chain_type, verbose, **kwargs)\u001b[0m\n\u001b[0;32m    141\u001b[0m \u001b[39mif\u001b[39;00m chain_type \u001b[39mnot\u001b[39;00m \u001b[39min\u001b[39;00m loader_mapping:\n\u001b[0;32m    142\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[0;32m    143\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mGot unsupported chain type: \u001b[39m\u001b[39m{\u001b[39;00mchain_type\u001b[39m}\u001b[39;00m\u001b[39m. \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    144\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mShould be one of \u001b[39m\u001b[39m{\u001b[39;00mloader_mapping\u001b[39m.\u001b[39mkeys()\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m\n\u001b[0;32m    145\u001b[0m     )\n\u001b[1;32m--> 146\u001b[0m \u001b[39mreturn\u001b[39;00m loader_mapping[chain_type](llm, verbose\u001b[39m=\u001b[39;49mverbose, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\jreno\\Documents\\Projects\\Mycoach Health\\venv\\Lib\\site-packages\\langchain\\chains\\summarize\\__init__.py:58\u001b[0m, in \u001b[0;36m_load_map_reduce_chain\u001b[1;34m(llm, map_prompt, combine_prompt, combine_document_variable_name, map_reduce_document_variable_name, collapse_prompt, reduce_llm, collapse_llm, verbose, token_max, **kwargs)\u001b[0m\n\u001b[0;32m     56\u001b[0m reduce_chain \u001b[39m=\u001b[39m LLMChain(llm\u001b[39m=\u001b[39m_reduce_llm, prompt\u001b[39m=\u001b[39mcombine_prompt, verbose\u001b[39m=\u001b[39mverbose)\n\u001b[0;32m     57\u001b[0m \u001b[39m# TODO: document prompt\u001b[39;00m\n\u001b[1;32m---> 58\u001b[0m combine_documents_chain \u001b[39m=\u001b[39m StuffDocumentsChain(\n\u001b[0;32m     59\u001b[0m     llm_chain\u001b[39m=\u001b[39;49mreduce_chain,\n\u001b[0;32m     60\u001b[0m     document_variable_name\u001b[39m=\u001b[39;49mcombine_document_variable_name,\n\u001b[0;32m     61\u001b[0m     verbose\u001b[39m=\u001b[39;49mverbose,\n\u001b[0;32m     62\u001b[0m )\n\u001b[0;32m     63\u001b[0m \u001b[39mif\u001b[39;00m collapse_prompt \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m     64\u001b[0m     collapse_chain \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\jreno\\Documents\\Projects\\Mycoach Health\\venv\\Lib\\site-packages\\langchain\\load\\serializable.py:74\u001b[0m, in \u001b[0;36mSerializable.__init__\u001b[1;34m(self, **kwargs)\u001b[0m\n\u001b[0;32m     73\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__init__\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs: Any) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m---> 74\u001b[0m     \u001b[39msuper\u001b[39;49m()\u001b[39m.\u001b[39;49m\u001b[39m__init__\u001b[39;49m(\u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[0;32m     75\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_lc_kwargs \u001b[39m=\u001b[39m kwargs\n",
      "File \u001b[1;32mc:\\Users\\jreno\\Documents\\Projects\\Mycoach Health\\venv\\Lib\\site-packages\\pydantic\\main.py:341\u001b[0m, in \u001b[0;36mpydantic.main.BaseModel.__init__\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mValidationError\u001b[0m: 1 validation error for StuffDocumentsChain\n__root__\n  document_variable_name text was not found in llm_chain input_variables: ['contextData', 'patientData'] (type=value_error)"
     ]
    }
   ],
   "source": [
    "#Parse nlp pipe through the csv doc loader\n",
    "from langchain.text_splitter import SpacyTextSplitter\n",
    "from langchain.embeddings.spacy_embeddings import SpacyEmbeddings\n",
    "from langchain.chains.summarize import load_summarize_chain\n",
    "from langchain.chains.mapreduce import MapReduceChain\n",
    "\n",
    "#Define the text splitter\n",
    "tokenizer = tiktoken.get_encoding('p50k_base')\n",
    "textSplitter = SpacyTextSplitter(pipeline=\"en_core_sci_md\")\n",
    "\n",
    "\n",
    "df = pd.read_csv(csvPath, nrows=1000)\n",
    "df = df.TEXT.to_list()\n",
    "\n",
    "chain = create_extraction_chain_pydantic(patient, llm)\n",
    "embedder = SpacyEmbeddings(nlp=en_core_sci_md.load())\n",
    "summaryChain = load_summarize_chain(llm, chain_type=\"map_reduce\", return_intermediate_results=True, map_prompt=prompt, combine_prompt=prompt)\n",
    "\n",
    "\n",
    "count, lim = 0, 10\n",
    "overallData = [] #List of Objects\n",
    "\n",
    "for item in df:\n",
    "    #Pass the items through nlp pipeline \n",
    "    if count == lim: break\n",
    "    \n",
    "    count += 1\n",
    "    splitText = textSplitter.split_text(item)\n",
    "    embeddings = embedder.embed_documents(splitText)\n",
    "    \n",
    "    # contextData = nlp.pipe(str(item), n_process=5) pipe or regular nlp parse ? Less time consuming but iterator object handling\n",
    "    contextData = nlp(str(item))\n",
    "    patientData = chain.run(embeddings)\n",
    "    \n",
    "    # embeddings = embedder.embed_documents(splitText)\n",
    "    contextData = contextData.to_json()\n",
    "    singlePatient = {**patientData, **contextData} # type: ignore\n",
    "    overallData = []    \n",
    "    summaryPatient = summaryChain.run(contextData, patientData)\n",
    "    \n",
    "    #Can embed queries and retrieve from an in-memory store but need storage after this for long term persistence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
